{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b509dbb932dc4899a62107353c54dbab",
     "grade": false,
     "grade_id": "cell-52f5559d92887e3c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Probabilistic Language Modeling\n",
    "\n",
    "In this assignment, you will implement a probabilistic language model based on n-grams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2c2d03143804d6e9efee123c09117205",
     "grade": false,
     "grade_id": "cell-289520bd6cca09dc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Probability\n",
    "\n",
    "In probability theory, the likelihood that an event (A) would occur is quantified by the formula $P(A)$ (the probability of A). If the event A is a coin flip resulting in *heads*, and we have data showing 3 coin flips have resulted in 1 heads and 2 tails, then we can calculate\n",
    "\n",
    "$P(A) = \\frac{\\# heads}{\\# coin flips} = \\frac{1}{3}$.\n",
    "\n",
    "In NLP, we use probability theory to reason about the likelihood of a word (or token) occurring in a sentence. So, if we have a corpus consisting of a single sentence \"I like avocados\", we can say the probability of the word \"avocados\" in that corpus is $\\frac{1}{3}$ because it appears once out of three words.\n",
    "\n",
    "### Helpful formulas\n",
    "\n",
    "#### Conditional Probability\n",
    "\n",
    "When reasoning about the likelihood of an event occurring, it makes sense that our reasoning might change if we have more context. For instance, if we know that some event has *already* occurred, we can look at our data and calculate the probability of another event occurring *given that we know* that first event occurred.\n",
    "\n",
    "This is the idea behind conditional probability. $P(A \\mid B)$ is the probability of event A given that event B has already occurred. In NLP, for example, if our data consists solely of the two sentences \"I like avocados\" and \"I hate avocados\", the probability of the word \"like\" occurring immediately after the word \"I\" is 1/2, or 0.5.\n",
    "\n",
    "$P(A \\mid B)=\\frac{P(A \\cap B)}{P(B)}$\n",
    "\n",
    "So, how do we reason about the conditional probability $P(like|I)$ in NLP? That is, the probability that the word \"like\" follows the word \"I\"? In ngram language modeling, we'll use word counts in our corpus:\n",
    "\n",
    "$P(like|I) = \\frac{P(like \\cap I)}{P(I)} = \\frac{count(\\text{\"I like\"})}{count(\\text{\"I\"})} = \\frac{1}{2}$\n",
    "\n",
    "In other words, there are two words that can follow \"I\" in the corpus: \"like\" and \"hate\". Since there is only one \"like\" in the corpus, the probability of \"like\" occurring after \"I\" is 1/2.\n",
    "\n",
    "#### Joint Probability\n",
    "\n",
    "To predict the probability of an *entire sentence* we would want to calculate the probability $P(w_1w_2w_3...w_n)$ for a sentence made up of $n$ words labeled $w_i$. This requires a slight modification to our previous equation as this probability would be akin to $P(X_1 \\cap X_2 \\cap X_3 \\cap ... X_n)$. This is called a joint probability, or the likelihood of some number of events co-occurring. In the simple case of two events, we can derive from our conditional probability formula:\n",
    "\n",
    "$P(A \\cap B)=P(A \\mid B) * P(A)$\n",
    "\n",
    "#### Chain Rule of Probability\n",
    "\n",
    "However, in the case of a longer sequence the appearance of each word is conditioned on the appearance of all prior words in the sentence. Thus, to find the probability of the entire sequence, we can use the chain rule of probability:\n",
    "\n",
    "$\\begin{aligned}\n",
    "P\\left(X_1 \\ldots X_n\\right) & =P\\left(X_1\\right) P\\left(X_2 \\mid X_1\\right) P\\left(X_3 \\mid X_{1: 2}\\right) \\ldots P\\left(X_n \\mid X_{1: n-1}\\right) \\\\\n",
    "& =\\prod_{k=1}^n P\\left(X_k \\mid X_{1: k-1}\\right)\n",
    "\\end{aligned}$\n",
    "\n",
    "Putting this equation in the context of our sentences:\n",
    "\n",
    "$\\begin{equation*}\n",
    "    \\begin{split}\n",
    "        P(w_{1:n}) & = P(w_1)P(w_2|w_1)P(w_3|w_{1:2})...P(w_n|w_{1:n-1}) \\\\\n",
    "        & = \\prod^n_{k=1} P(w_k|w_{1:k-1})\n",
    "    \\end{split}\n",
    "\\end{equation*}$\n",
    "\n",
    "For a clearer idea of why we use these formulas in for ngram language modeling, review page 4, chapter 3 of Jurafsky and Martin - https://web.stanford.edu/~jurafsky/slp3/3.pdf. Now, let's start putting some of these formulas into practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6698ec5a96c2b0e139b35b3c191322b3",
     "grade": false,
     "grade_id": "cell-985e2efe280d2971",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Unigram Language Modeling\n",
    "\n",
    "Let's start by implementing the functions we need for a unigram language model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "45d0fbf7aa7092b763673337ae422fb8",
     "grade": false,
     "grade_id": "cell-c0c0f37abcbb0af0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from typing import List, Union, Tuple, Dict, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9effc35cde59f32ec7ae63c55c8f414e",
     "grade": false,
     "grade_id": "cell-634e4dc49ef30db5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# sample corpus ###################################################################\n",
    "corpus = ['<s> Man was I wrong. </s>',\n",
    "          '<s> This place is terrible. </s>',\n",
    "          '<s> The room was very dirty and there were dead bugs everywhere. </s>',\n",
    "          '<s> What was I thinking? </s>',\n",
    "          '<s> This place was cheap and I thought I was clever. </s>',\n",
    "          '<s> Man was I wrong. </s>']\n",
    "###################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "974a959977e6a987632afc78eb337ea4",
     "grade": false,
     "grade_id": "cell-61e724568baa5dba",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Preprocessing\n",
    "\n",
    "Working with text data in the `str` format gets very inefficient as we scale up the size of the dataset. Although our sample corpus is small, let's just mitigate this problem right off the bat by tokenizing our corpus and converting all `str`s to `int`s. We'll call each `int` that corresponds to a unique token the `token_id` or just `id`. We'll keep track of our vocabulary and token IDs so that we can easily convert and re-convert sequences. This is a common way to preprocess data in NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ead6d15b77186a6a337f659bf75ffa19",
     "grade": false,
     "grade_id": "cell-2ca52e404e92b04b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def convert_strs_to_ids(strings: List[str])-> Tuple[List[List[int]], defaultdict]:\n",
    "    \"\"\"Convert a corpus of strings to a tokenized corpus\n",
    "    of integer token_ids\"\"\"\n",
    "    token_to_id = defaultdict(lambda: len(token_to_id))  # Dictionary to map tokens to unique IDs\n",
    "    \n",
    "    tokenized_strings = []  # List to hold tokenized strs as lists of IDs\n",
    "    \n",
    "    for string in strings:\n",
    "        token_ids = []  # List to hold IDs for each token in the current string\n",
    "        tokens = string.split()  # Tokenize the string by splitting on spaces\n",
    "        for token in tokens:\n",
    "            token_ids.append(token_to_id[token])  # Automatically assign ID if token is new\n",
    "        tokenized_strings.append(token_ids)\n",
    "    \n",
    "    return tokenized_strings, token_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3bef5fbc822071a38b65fd04eda2655f",
     "grade": false,
     "grade_id": "cell-99791c6c9e2512e0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "tokenized_corpus, token_to_id = convert_strs_to_ids(corpus) # Tokenize and convert our corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4c17ca247b82ced1a8ba975beabc3281",
     "grade": false,
     "grade_id": "cell-5bed705ebf45aaad",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Create a reverse mapping from ID to token\n",
    "id_to_token = {id_: token for token, id_ in token_to_id.items()}\n",
    "print(tokenized_corpus)\n",
    "print([id_to_token[token_id] for token_id in tokenized_corpus[0]]) # Reconverting back to check that it works..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cfd2c9cb7b5e92441c9e28aa6050dda5",
     "grade": false,
     "grade_id": "cell-65e0a9d863462640",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Looks like our tokenizing and converting works as intended. We see our special start token `<s>` has `token_id = 0`, the token `Man` has `token_id = 1`, etc. Using these special start-of-sentence and end-of-sentence tokens `<s>` and `</s>` are a common practice in NLP, and the end token in particular will help us generate fluent-ish sentences in this assignment.\n",
    "\n",
    "Now, let's calculate probabilities based on our tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "91c0d18f5a7127d6f0e19986d904c16b",
     "grade": false,
     "grade_id": "cell-b028f34ad0c3f8f5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def unigram_probability(word: int, corpus: List[List[int]]) -> float:\n",
    "    \"\"\"\n",
    "    Return the probability of `word` in `corpus`.\n",
    "    P(w_1) = count(w_1) / N\n",
    "    \"\"\"\n",
    "    \n",
    "    # Below is a list of token IDs for all sentences\n",
    "    # in the corpus, and a dictionary of counts for each unique word.\n",
    "    tokens = [token for tokenized_sentence in corpus for token in tokenized_sentence]\n",
    "    counts = Counter(tokens)\n",
    "    \n",
    "    # Find the probability of word given these counts\n",
    "    probability = counts[word] / len(tokens)\n",
    "    \n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "da51989c06ce60a61b51a585de57b90d",
     "grade": false,
     "grade_id": "cell-ec7859188373645f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# There are 49 tokens in our sample corpus, and \"was\"\n",
    "# appears 6 times, so our probability should equal 6/49...\n",
    "print('P(\"was\") =', unigram_probability(token_to_id[\"was\"], tokenized_corpus))\n",
    "print('6/49 =', 6/49)\n",
    "if unigram_probability(token_to_id[\"was\"], tokenized_corpus) == 6/49:\n",
    "    print(\"Woohoo, it works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams and trigrams\n",
    "\n",
    "Now, let's implement bigram and trigram probabilities so that we can create our bigram and trigram language models. First, write function a to get a list of bigrams, and a function to get a list of trigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f43d7062786595f5e46d830fabbc53a3",
     "grade": false,
     "grade_id": "cell-1a41b7de6b585bd8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_bigrams(corpus: List[List[int]]) -> List[Tuple[int, int]]:\n",
    "    \"\"\"Return a list of bigrams in `corpus`.\"\"\"\n",
    "    \n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "    \n",
    "    return bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b4c199663c7aa93f1cc6035760707f75",
     "grade": true,
     "grade_id": "cell-f442a8fcec96b76b",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "sample_bigrams = get_bigrams(tokenized_corpus)\n",
    "all_bigrams = [(0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (0, 6), (6, 7), (7, 8), (8, 9), (9, 5), (0, 10), (10, 11), (11, 2), (2, 12), (12, 13), (13, 14), (14, 15), (15, 16), (16, 17), (17, 18), (18, 19), (19, 5), (0, 20), (20, 2), (2, 3), (3, 21), (21, 5), (0, 6), (6, 7), (7, 2), (2, 22), (22, 14), (14, 3), (3, 23), (23, 3), (3, 2), (2, 24), (24, 5), (0, 1), (1, 2), (2, 3), (3, 4), (4, 5)]\n",
    "for bigram in all_bigrams:\n",
    "    assert bigram in sample_bigrams\n",
    "print('All test cases have passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "03ff91b7595d196a1a0fc90a2e6e2ba1",
     "grade": false,
     "grade_id": "cell-ad25ab507b73ff62",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now that we have all of the bigrams, we can compute the probability of any word occurring after another as follows:\n",
    "\n",
    "$P(\\text{word2}|\\text{word1}) = \\frac{count(\\text{word1}, \\text{word2})}{count(\\text{word1})}$, where $count(\\text{word1}, \\text{word2})$ is the number of times $\\text{word1}$ and $\\text{word2}$ occur together and $count(\\text{word1})$ is the number of times $\\text{word1}$ occurs. (This uses the same logic as the conditional probability we reviewed earlier.)\n",
    "\n",
    "Write a function to compute the conditional probability of a word given another word. The function should take a corpus and two words as input and return the conditional probability of the second word given the first word. For example, the probability of the word \"quick\" occurring given that the word \"the\" has already occurred is P(quick|the).\n",
    "\n",
    "The function should also implement Laplace (add-one) smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eaa517c539b392d74d13c083ac9c6e10",
     "grade": false,
     "grade_id": "cell-f5ca0f775de0e0c4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def conditional_prob(w_2, w_1, corpus: List[List[int]]) -> float:\n",
    "    \"\"\"Return the conditional probability of `w_2` given `w_1` in `corpus`.\n",
    "    P(w_2 | w_1) = count(w_1, w_2) / count(w_1)\n",
    "    \"\"\"\n",
    "    \n",
    "    bigrams = get_bigrams(corpus)\n",
    "    tokens = [token for tokenized_sentence in corpus for token in tokenized_sentence]\n",
    "    unigram_counts = Counter(tokens)\n",
    "    bigram_counts = Counter(bigrams)\n",
    "    \n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "    \n",
    "    return conditional_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a30558cdb945967d2b4719ffc277b83a",
     "grade": true,
     "grade_id": "cell-aa44e9b52356a908",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# There are 6 instances of the unigram \"<s>\" and\n",
    "# 2 of those instances coincide with the bigram \"<s> Man\"\n",
    "# So the probability of \"<s> Man\" should be 2/6 = 1/3...\n",
    "assert conditional_prob(token_to_id[\"Man\"], token_to_id[\"<s>\"], tokenized_corpus) == 0.3333333333333333\n",
    "print(\"Woohoo, it works!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "903a790eefcd797f41d5b24893a70a00",
     "grade": false,
     "grade_id": "cell-96ef6f870cd1928e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# But what if we want the probability of an out-of-vocabulary token?\n",
    "print(conditional_prob(token_to_id[\"cat\"], token_to_id[\"the\"], tokenized_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0a767569cf7884ccf42864a46bcfc280",
     "grade": false,
     "grade_id": "cell-a317472d0716c7b7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Laplace Smoothing\n",
    "This is looking good but we're getting a division by zero error! Let's use *Laplace Smoothing* to fix it. \n",
    "\n",
    "Laplace smoothing is a technique for dealing with words that do not occur in the corpus. It is a way of adjusting the probability of a word occurring given another word by adding 1 to the numerator and the number of unique words in the corpus to the denominator. This ensures that the probability of a word occurring is never 0. For a clearer idea of why this formula works, refer page 6, chapter 3 of Jurafsky and Martin - https://web.stanford.edu/~jurafsky/slp3/3.pdf\n",
    "\n",
    "To implement Laplace Smoothing, we need to modify our conditional probability function as follows:\n",
    "\n",
    "$P\\left(\\text{word2}|\\text{word1} \\right)_{Laplace} = \\frac{count(\\text{word1},\\text{word2})+1}{count(\\text{word1})+V}$\n",
    "\n",
    "where $count(\\text{word1},\\text{word2})$ is the number of times $\\text{word1}$ and $\\text{word2}$ occur together, $count(\\text{word1})$, is the number of times $\\text{word1}$ occurs, and $V$ is the number of unique words in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "602618059f6cb618fab8bbda01062d0a",
     "grade": false,
     "grade_id": "cell-1710307d6e7195d0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Here's a handy helper function for calculating vocabulary size!\n",
    "# You can use this for Laplace (add-one) smoothing.\n",
    "def vocabulary_size(tokens: List[int]) -> int:\n",
    "    \"\"\"Return the vocabulary size of `tokens`. (Unique tokens = types)\"\"\"\n",
    "    return len(set(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "72d4def5b3b24de449017863556102b5",
     "grade": false,
     "grade_id": "cell-762db42f04ee5307",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def conditional_prob(w_2, w_1, corpus: List[List[int]]) -> float:\n",
    "    \"\"\"Return the conditional probability of `w_2` given `w_1` in `corpus`.\n",
    "    P(w_2 | w_1)_Laplace = count(w_1, w_2) + 1 / count(w_1) + V\n",
    "    Uses Laplace (add-one) smoothing.\n",
    "    \"\"\"\n",
    "    \n",
    "    bigrams = get_bigrams(corpus)\n",
    "    tokens = [token for tokenized_sentence in corpus for token in tokenized_sentence]\n",
    "    unigram_counts = Counter(tokens)\n",
    "    bigram_counts = Counter(bigrams)\n",
    "    \n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "    \n",
    "    return conditional_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8296b1163a4717d068af797af8a623ad",
     "grade": true,
     "grade_id": "cell-0dfa6e67cf58412f",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Now let's try that OOV word...\n",
    "assert conditional_prob(token_to_id[\"cat\"], token_to_id[\"the\"], tokenized_corpus) == 0.04\n",
    "print('Woohoo it works!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "359bb9c49ea386d9f3490d5f50dbaff4",
     "grade": false,
     "grade_id": "cell-1b97498089860033",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now, let's try and predict the next word in a sentence. This is essentially what an advanced chatbot like ChatGPT also tries to do, although ChatGPT uses a much bigger corpus and a more sophisticated algorithm. We will use the same corpus as before, but this time we will use the conditional probability formula to predict the next word in a sentence. We will also use the start and end tokens to help us compute the probabilities.\n",
    "\n",
    "<b>Write a function to predict the next word in a sentence. The function should take a corpus and a sentence as input and return the most likely next word in the sentence. For example, if the sentence is \"the quick brown\", the function should return \"fox\".</b>\n",
    "\n",
    "<i>Hint: You can use the conditional probability function you wrote earlier to compute the probability of each word in the vocabulary occurring after the last word in the sentence. The word with the highest probability is the most likely next word. In other words, check the probability of each possible word in the corpus with respect to the given previous word and pick the one with the maximum probability.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3c4da459fc1e511457fc464ca1ac78aa",
     "grade": false,
     "grade_id": "cell-6a0e7fc8594ff1db",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def predict_next_word(w_1, corpus) -> int:\n",
    "    \"\"\"Return the most likely next word given `w_1` in `corpus`.\"\"\"\n",
    "    \n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "    \n",
    "    return most_likely_next_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cd538b4d37f7537c9918f59a48a803ab",
     "grade": true,
     "grade_id": "cell-859f8054d0c69ab4",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert predict_next_word(token_to_id['was'], tokenized_corpus) == 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try and predict an entire sentence now.\n",
    "\n",
    "Write a function that takes an initial string and a word limit as input and returns a sentence no more than `limit` tokens longer than the original sequence. The initial string is the starting point for the sentence. For example, if the initial string is \"the quick brown\" and the word limit is 5, the function should return \"the quick brown fox jumped over the lazy\". The function should use the `predict_next_word` function you wrote earlier to predict the next word in the sentence. \n",
    "\n",
    "The function should automatically stop when it encounters a `</s>` (end of sequence) token, even if it encounters that token before the limit is reached.\n",
    "\n",
    "<i>Hint: You can use a for loop to predict the next word in the sentence. The initial string is the starting point for the sentence. At each iteration, you can add the predicted word to the sentence and use the predicted word as the new initial string. You can stop the loop when the word limit is reached. </i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4dc53acb1e5eb353a77913f0582fd1e6",
     "grade": false,
     "grade_id": "cell-455f4e7d300d575e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "stop_token_id = token_to_id['</s>']\n",
    "def predict_sentence_bigrams(initial_sequence: List[int], corpus: List[List[int]], limit: int = 5) -> List[int]:\n",
    "    \"\"\"Return a sentence of `len(initial_sequence)`+`limit` words \n",
    "    predicted from `initial_sequence` in `corpus`.\"\"\"\n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "    return predicted_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "171dc982abe4c0980fa61eb6d6eec7ca",
     "grade": true,
     "grade_id": "cell-fa70addd89213eb7",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "id_to_token = {id_: token for token, id_ in token_to_id.items()}\n",
    "initial_sequence = [token_to_id[word] for word in \"This is\".split()]\n",
    "predicted_sequence = predict_sentence_bigrams(initial_sequence, tokenized_corpus, limit = 5)\n",
    "reconstructed_sentence = ' '.join([id_to_token[id_] for id_ in predicted_sequence])\n",
    "assert reconstructed_sentence == 'This is terrible.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity\n",
    "Perplexity is a measurement in NLP used to evaluate language models. It's a measure of how well a probability model predicts a sample. A lower perplexity score indicates better performance of the model.\n",
    "\n",
    "The perplexity of a sentence is calculated as the inverse probability of the sentence, normalized by the number of words. In other words, it's the geometric mean of the inverse conditional probability of each word given the previous word in the sentence.\n",
    "\n",
    "$\\text{Perplexity}(W)=\\sqrt[N]{\\frac{1}{P(w_1, w_2, \\ldots, w_N)}}=\\sqrt[N]{\\prod_{i=1}^N \\frac{1}{P(w_i \\mid w_1, \\ldots, w_{i-1})}}$\n",
    "\n",
    "where $N$ is the number of words in the sentence and $P(w_i \\mid w_1, \\ldots, w_{i-1})$ is the conditional probability of the i'th word given the previous words in the sentence.\n",
    "\n",
    "In our case, since we only have bigrams, the formula becomes:\n",
    "$\\text{Perplexity}(W)=\\sqrt[N]{\\frac{1}{P(w_1, w_2, \\ldots, w_N)}}=\\sqrt[N]{\\prod_{i=1}^N \\frac{1}{P(w_i \\mid w_{i-1})}}$\n",
    "\n",
    "Perplexity is a useful metric for evaluating language models because it is a measure of how well a probability model predicts a sample. A lower perplexity score indicates better performance of the model.\n",
    "\n",
    "Let's try and calculate the perplexity. We will use the same corpus as before, but this time we will use the perplexity formula to calculate the perplexity of each sentence. We will also use the start and end tokens to help us compute the perplexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2e7fb989b69870a10415f83129bfc63d",
     "grade": false,
     "grade_id": "cell-0da9e58825aa2897",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "NOTE: We have a problem when finding the conditional probability of $w_1$ here as there are no preceding tokens. To make life simple, we can use the standard probability of that word occurring over the *entire corpus*. We can use the `unigram_probability` function to find the probability of $w_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "775356eae8d33007b5e70b156706bd9c",
     "grade": false,
     "grade_id": "cell-2396eb58887cb484",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def perplexity_bg(test_sequence: List[int], corpus: List[List[int]]) -> float:\n",
    "    \"\"\"Return the perplexity of `test_sequence` given `corpus` using bigrams.\"\"\"\n",
    "    product = 1\n",
    "    # First word\n",
    "    product *= (1 / unigram_probability(test_sequence[0], corpus))\n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "94ec95bec1e013f211ac34dbc471cbb3",
     "grade": false,
     "grade_id": "cell-55eab460af8d5d54",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now run the perplexity calculation on a couple of sample sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f59f6741f4aba22fb3a8c9a021c8460c",
     "grade": false,
     "grade_id": "cell-2f19b04d3bc87219",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test_seq1 = [token_to_id[word] for word in \"The quick brown fox jumped over the lazy dog\".split()]\n",
    "test_seq2 = [token_to_id[word] for word in \"The quick brown fox jumped over the perplexing dog\".split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(perplexity_bg(\n",
    "    test_seq1, \n",
    "    [test_seq1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "240fb2755e8e6b234d1964b016159475",
     "grade": true,
     "grade_id": "cell-747ef3db8d05732c",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert perplexity_bg(\n",
    "    test_seq1, \n",
    "    [test_seq1]) < perplexity_bg(\n",
    "    test_seq1, \n",
    "    [test_seq2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a258af93933b80b6c02b8dc00c5b4511",
     "grade": false,
     "grade_id": "cell-cbdb5610f747a668",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now we'll improve the model by using trigrams instead of bigrams.\n",
    "\n",
    "\n",
    "A trigram is a tuple of three words. For example, the sentence \"The quick brown fox jumps over the lazy dog\" contains 7 trigrams: (The, quick, brown), (quick, brown, fox), (brown, fox, jumps), (fox, jumps, over), (jumps, over, the), (over, the, lazy), (the, lazy, dog). You can use the same formula to compute the conditional probability of a word given two other words. You can also use the start and end tokens to help you compute the probabilities. \n",
    "\n",
    "You can use the same function you wrote earlier to compute the conditional probability of a word given two other words. The function should take a corpus and three words as input and return the conditional probability of the third word given the first two words. For example, the probability of the word \"the\" occurring given that the words \"quick\" and \"brown\" have already occurred is P(the|quick, brown)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "43f42eeab57221898db2662cb7337640",
     "grade": false,
     "grade_id": "cell-d03704aeb7e8fc9a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_trigrams(corpus: List[List[int]]) -> List[Tuple[int, int, int]]:\n",
    "    \"\"\"Return a list of trigrams in `corpus`.\"\"\"\n",
    "    \n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "\n",
    "    return trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8d2af7d108002e903adf7c86d9e0edfc",
     "grade": true,
     "grade_id": "cell-3cd75dadf4b96d7a",
     "locked": true,
     "points": 8,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "trigrams = get_trigrams(tokenized_corpus)\n",
    "assert (1, 2, 3) in trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6f4d2723b748e272c18fbd68c3ffc3e1",
     "grade": false,
     "grade_id": "cell-ae002492fe0dbe4b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now update the conditional probability function to handle trigrams. Then, predict the next word, and a full sentence, just like we did with the bigram model. Remember to use Laplace (add-one) smoothing to handle out-of-vocabulary words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "250df107a56dcfba032ba479ec438ffb",
     "grade": false,
     "grade_id": "cell-31ce8ebb6d3658cc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def conditional_prob_trigram(w_3, w_2, w_1, corpus: List[List[int]]) -> float:\n",
    "    \"\"\"Return the conditional probability of `w_3` given `w_2` and `w_1` in `corpus`.\n",
    "    P(w_3 | w_2, w_1)_Laplace = count(w_1, w_2, w_3) + 1 / count(w_1, w_2) + V\n",
    "    Uses Laplace (add-one) smoothing.\n",
    "    \"\"\"\n",
    "    \n",
    "    tokens = [token for tokenized_sentence in corpus for token in tokenized_sentence]\n",
    "    bigrams = get_bigrams(corpus)\n",
    "    trigrams = get_trigrams(corpus)\n",
    "    \n",
    "    bigram_counts = Counter(bigrams)\n",
    "    trigram_counts = Counter(trigrams)\n",
    "    \n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "    \n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8a65df7f86a1ecd264cdd99102c97908",
     "grade": true,
     "grade_id": "cell-f5ea83b99cb1ba6f",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert conditional_prob_trigram(\n",
    "    token_to_id['wrong'], \n",
    "    token_to_id['I'], \n",
    "    token_to_id['was'], \n",
    "    tokenized_corpus) == 0.03571428571428571"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "326e8533434721bad3a2633f8b67853c",
     "grade": false,
     "grade_id": "cell-52a471c6b2eed74d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def predict_next_word_trigram(w_2, w_1, corpus: List[List[int]]) -> int:\n",
    "    \"\"\"Return the most likely next word given `w_2` and `w_1` in `corpus`.\"\"\"\n",
    "    \n",
    "    # your code here\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "82d8d66efb3ec95eb69663131d08462e",
     "grade": true,
     "grade_id": "cell-3b04c6b159e75e7d",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert predict_next_word_trigram(\n",
    "        token_to_id['very'], \n",
    "        token_to_id['was'], \n",
    "        tokenized_corpus) == 13 # 13 is 'dirty'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "29ba6a4d7cf52e92aed73c5f346bd6f9",
     "grade": false,
     "grade_id": "cell-fc0e8bdd5a3e1d80",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now generate a full sentence and calculate the perplexity using the trigram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0d43aa99afd31f468da1894820a160af",
     "grade": false,
     "grade_id": "cell-fe42f9b0a3eddbf6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "stop_token_id = token_to_id['</s>']\n",
    "def predict_sentence_trigrams(initial_sequence: List[int], corpus: List[List[int]], limit: int = 5) -> List[int]:\n",
    "    \"\"\"Return a sentence of `len(initial_sequence)`+`limit` words \n",
    "    predicted from `initial_sequence` in `corpus`.\"\"\"\n",
    "\n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "    return predicted_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e04d44a8057192e0161bc981a5733ea4",
     "grade": true,
     "grade_id": "cell-6371b85882fed42d",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "id_to_token = {id_: token for token, id_ in token_to_id.items()}\n",
    "initial_sequence = [token_to_id[word] for word in \"Man was\".split()]\n",
    "predicted_sequence = predict_sentence_trigrams(initial_sequence, tokenized_corpus, limit = 5)\n",
    "reconstructed_sentence = ' '.join([id_to_token[id_] for id_ in predicted_sequence])\n",
    "assert reconstructed_sentence == 'Man was I wrong.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3579655e72ba94fac677be1e4a5671f2",
     "grade": false,
     "grade_id": "cell-575a9f8eb55a910a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now create a function that calculates perplexity based on the trigram model. \n",
    "\n",
    "NOTE: Remember in `perplexity_bg` how we used the `unigram_probability` function to calculate the probability of $w_1$ (i.e. the edge case where there are no preceding tokens)? Well, we'll also have to use the `conditional_prob` (bigram probability) function in the $w_2$ case for trigrams, when there is only *one* preceding token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a629e3cf25588253ccbe5e94987a7fcf",
     "grade": false,
     "grade_id": "cell-9215f6e6ab79129e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def perplexity_tg(test_sequence: List[int], corpus: List[List[int]]) -> float:\n",
    "    \"\"\"Return the perplexity of `test_sequence` given `train_corpus` using trigrams.\"\"\"\n",
    "\n",
    "    product = 1\n",
    "    # First word\n",
    "    product *= 1 / unigram_probability(test_sequence[0], corpus)\n",
    "    # First two words\n",
    "    product *= 1 / conditional_prob(test_sequence[1], test_sequence[0], corpus)\n",
    "    # your code here\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "950b68796eca6ac549cd7371c7c7cb8e",
     "grade": false,
     "grade_id": "cell-8833059659a13a82",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(perplexity_tg(test_seq2, [test_seq1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9005bde99a6a970d8391f84aa47af41b",
     "grade": true,
     "grade_id": "cell-dbee7ac3824a2e4c",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert perplexity_tg(test_seq1, \n",
    "    [test_seq1]) < perplexity_tg(\n",
    "    test_seq2, \n",
    "    [test_seq1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLT Corpus\n",
    "\n",
    "Now let's try this with a real corpus. We will use the Boulder Lies and Truth (BLT) corpus. Boulder Lies and Truth was developed at the University of Colorado Boulder and contains approximately 1,500 elicited English reviews of hotels and electronics for the purpose of studying deception in written language. Reviews were collected by crowd-sourcing with Amazon Mechanical Turk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "af08046891b6ff1358f25bdace3eb67b",
     "grade": false,
     "grade_id": "cell-d6f265dc04fc73c5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's get the reviews and the sentiment of each review,\n",
    "# and then explore some of the data.\n",
    "import csv\n",
    "\n",
    "file_path = 'blt_corpus.csv'\n",
    "reviews = []\n",
    "sentiment = []\n",
    "\n",
    "with open(file_path, mode='r', encoding='utf-8') as file:\n",
    "    reader = csv.DictReader(file) \n",
    "    for row in reader:\n",
    "        reviews.append(row['Review'])\n",
    "        sentiment.append(row['Sentiment Polarity'])\n",
    "\n",
    "# Add start and end tags to each review\n",
    "reviews = ['<s> ' + sentence + ' </s>' for sentence in reviews][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "345cba92005455ba511242720649b9cc",
     "grade": false,
     "grade_id": "cell-b3dbe120eac54ec0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Data exploration and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "81b689aba5c232f81f3c7ab8dee79fdb",
     "grade": false,
     "grade_id": "cell-48054613dbde006d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "reviews[0] # the first review is... hard to decipher..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2d1d46ede91da409815378b55a754eaa",
     "grade": false,
     "grade_id": "cell-bb5fd43c8f42e5ea",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "reviews = reviews[1:] # let's get rid of that review.\n",
    "sentiment = sentiment[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6d38e5275ba68466c24c0b048c70c331",
     "grade": false,
     "grade_id": "cell-9c50e9119c969882",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print('Review:', reviews[0]) # okay, the start and end tokens are there\n",
    "print('Sentiment:', sentiment[0]) # let's check that the sentiment of this review makes sense too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "89aceeac5a660daa871682dba89f5cf2",
     "grade": false,
     "grade_id": "cell-9cb6fcda3cfc4d39",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Now let's tokenize and convert these reviews to IDs as before\n",
    "tokenized_reviews, token_to_id = convert_strs_to_ids(reviews) # Tokenize and convert our corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b439d993e5ca2aa3a2a03ee74ffb1b4d",
     "grade": false,
     "grade_id": "cell-b7e1440a8d7d0727",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Sanity check...\n",
    "print('IDs:', tokenized_reviews[0])\n",
    "id_to_token = {id_: token for token, id_ in token_to_id.items()}\n",
    "print('Reconverted:', ' '.join([id_to_token[id_] for id_ in tokenized_reviews[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ee4276b6ad86473df83edc0c6074d71c",
     "grade": false,
     "grade_id": "cell-a4092b85c8a9f026",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now let's try to make a sentence from our models trained on the BLT corpus. This may take some time to run, so let's just play around with a sample of the corpus of size 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "efccbe448b8dce1c410b250f3c3ee9dd",
     "grade": false,
     "grade_id": "cell-88193490bdb532a9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "sentence = \"I\"\n",
    "initial_sequence = [token_to_id[word] for word in sentence.split()]\n",
    "predicted_sequence = predict_sentence_bigrams(initial_sequence, tokenized_reviews[:10], limit = 3)\n",
    "reconstructed_sentence = ' '.join([id_to_token[id_] for id_ in predicted_sequence])\n",
    "print('Predicted sentence:', reconstructed_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# That wasn't very fluent. Let's try with trigrams...\n",
    "# Note that this will take longer.\n",
    "sentence = \"I have\"\n",
    "initial_sequence = [token_to_id[word] for word in sentence.split()]\n",
    "predicted_sequence = predict_sentence_trigrams(initial_sequence, tokenized_reviews[:10], limit = 3)\n",
    "reconstructed_sentence = ' '.join([id_to_token[id_] for id_ in predicted_sequence])\n",
    "print('Predicted sentence:', reconstructed_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9703ad15c48d84f28c2ada640d699a8c",
     "grade": false,
     "grade_id": "cell-658444b0f9f8de83",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The trigram model is a bit more fluent, even when we only train on 10 reviews!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "809ce29bd77ac12f63bc393d25aad780",
     "grade": false,
     "grade_id": "cell-5e81519fad73dad3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Faster and more flexible functions\n",
    "\n",
    "One reason our functions are so slow is because they are continually calling `get_bigrams` and `get_trigrams`. Let's rewrite them so that they take in bigrams and trigrams as arguments, instead of an unprocessed corpus. And, to cut down on the amount of code we have to write, let's make our functions flexible, so that they accept an ngram of $n \\in [1,2,3]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e2014d4a57e80b8bba009892ea4d0f9a",
     "grade": false,
     "grade_id": "cell-1379b916b8dfa160",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Helpful classes!\n",
    "class Ngram:\n",
    "    def __init__(self, w_1: int, w_2: Optional[int] = None, w_3: Optional[int] = None):\n",
    "        \"\"\"Initialize an Ngram with up to three words (IDs).\"\"\"\n",
    "        self.w_1 = w_1\n",
    "        self.w_2 = w_2\n",
    "        self.w_3 = w_3\n",
    "\n",
    "    @property\n",
    "    def n(self) -> int:\n",
    "        \"\"\"Return the \"n\" value of the n-gram (1 for unigram, 2 for bigram, 3 for trigram).\"\"\"\n",
    "        if self.w_3 is not None:\n",
    "            return 3\n",
    "        elif self.w_2 is not None:\n",
    "            return 2\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    def as_tuple(self) -> Tuple[int, ...]:\n",
    "        \"\"\"Return the n-gram as a tuple of integers.\"\"\"\n",
    "        if self.n == 3:\n",
    "            return (self.w_1, self.w_2, self.w_3)\n",
    "        elif self.n == 2:\n",
    "            return (self.w_1, self.w_2)\n",
    "        else:\n",
    "            return (self.w_1,)\n",
    "        \n",
    "    def __hash__(self):\n",
    "        return hash(self.as_tuple())\n",
    "        \n",
    "    def __eq__(self, other):\n",
    "        \"\"\"Check equality of two Ngram instances.\"\"\"\n",
    "        if isinstance(other, Ngram):\n",
    "            return self.as_tuple() == other.as_tuple()\n",
    "        return False\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        \"\"\"Return a string representation of the Ngram.\"\"\"\n",
    "        return f\"Ngram(n={self.n}, words={self.as_tuple()})\"\n",
    "    \n",
    "class NgramCorpus():\n",
    "    def __init__(self, n: int, tokenized_data: List[List[int]]):\n",
    "        \"\"\"A handy object to store all our ngram information in one place.\"\"\"\n",
    "        self.n = n\n",
    "        \n",
    "        if self.n >= 1:\n",
    "            self.unigrams = [Ngram(id_) for sequence in tokenized_data for id_ in sequence]\n",
    "            self.total_num_tokens_in_corpus = len(self.unigrams)\n",
    "            self.unigram_counts = Counter(self.unigrams)\n",
    "            self.v = len(self.unigram_counts.keys())\n",
    "        if self.n >= 2:\n",
    "            self.bigrams = [Ngram(*bg) for bg in get_bigrams(tokenized_data)]\n",
    "            self.bigram_counts = Counter(self.bigrams)\n",
    "        if self.n >= 3:\n",
    "            self.trigrams = [Ngram(*tg) for tg in get_trigrams(tokenized_data)]\n",
    "            self.trigram_counts = Counter(self.trigrams)\n",
    "\n",
    "    def count(self, ngram: Ngram) -> int:\n",
    "        \"\"\"Get the count of a specific N-gram.\"\"\"\n",
    "        \n",
    "        if ngram.n == 1:\n",
    "            return self.unigram_counts[ngram]\n",
    "        if ngram.n == 2:\n",
    "            return self.bigram_counts[ngram]\n",
    "        elif ngram.n == 3:\n",
    "            return self.trigram_counts[ngram]\n",
    "        else:\n",
    "            print('Error')\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7ba15228f4ec076de52d68d393c06630",
     "grade": false,
     "grade_id": "cell-abf343a87cac9b58",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# One ngram corpus to rule them all.\n",
    "# This will contain all unigrams, bigrams, and trigrams, \n",
    "# and all their counts in the BLT corpus.\n",
    "blt_ngram_corpus = NgramCorpus(n = 3, tokenized_data = tokenized_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bb5665061ab5663357a9ce839da0ba3a",
     "grade": false,
     "grade_id": "cell-0a9044d4087e82fc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Now let's rewrite our probability functions into one flexible function \n",
    "# using our Ngram and NgramCorpus data types.\n",
    "\n",
    "def ngram_probability(ngram: Ngram, ngram_corpus: NgramCorpus) -> float:\n",
    "    \"\"\"Return the conditional probability of `ngram_sequence` in `ngram_corpus`.\n",
    "    Assume \n",
    "    Remember:\n",
    "    P(w_1)_Laplace = count(w_1) + 1 / N + V\n",
    "    P(w_2 | w_1)_Laplace = count(w_1, w_2) + 1 / count(w_1) + V\n",
    "    P(w_3 | w_2, w_1)_Laplace = count(w_1, w_2, w_3) + 1 / count(w_1, w_2) + V\n",
    "    Uses Laplace (add-one) smoothing.\n",
    "    \"\"\"\n",
    "    # Vocabulary size (unique Ngrams)\n",
    "    vocab_size = ngram_corpus.v\n",
    "    if ngram.n == 1:\n",
    "        ngram_counts = ngram_corpus.unigram_counts\n",
    "    elif ngram.n == 2:\n",
    "        ngram_counts = ngram_corpus.bigram_counts\n",
    "        lower_order_ngram_counts = ngram_corpus.unigram_counts\n",
    "    elif ngram.n == 3:\n",
    "        ngram_counts = ngram_corpus.trigram_counts\n",
    "        lower_order_ngram_counts = ngram_corpus.bigram_counts\n",
    "    else:\n",
    "        print('Error')\n",
    "        return 0\n",
    "    \n",
    "    # Calculate the probability\n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "    \n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7c2d2bc8b6bf00b36cab38aa0620fcc5",
     "grade": true,
     "grade_id": "cell-241169175326f237",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Testing our new machinery...\n",
    "test_sequence_1 = 'I have never'\n",
    "test_sequence_2 = 'egyd hr fhjfhjtjr'\n",
    "ids1 = [token_to_id[w] for w in test_sequence_1.split()]\n",
    "ids2 = [token_to_id[w] for w in test_sequence_2.split()]\n",
    "ngram1 = Ngram(*ids1)\n",
    "ngram2 = Ngram(*ids2)\n",
    "\n",
    "assert ngram_probability(ngram1, blt_ngram_corpus) > ngram_probability(ngram2, blt_ngram_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2ef6b04a21cddfb485e7f7a2170cbe39",
     "grade": false,
     "grade_id": "cell-52649235806b1914",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def predict_next_word(context: Ngram, ngram_corpus: NgramCorpus) -> int:\n",
    "    \"\"\"Predict the most likely next word given the context Ngram using ngram_corpus.\"\"\"\n",
    "    if context.n not in [1, 2]:\n",
    "        raise ValueError(\"Context must be a unigram or bigram.\")\n",
    "\n",
    "    # Get the list of possible next words\n",
    "    if context.n == 1:\n",
    "        candidates = [Ngram(context.w_1, next_word.w_1) for next_word in ngram_corpus.unigram_counts.keys()]\n",
    "    elif context.n == 2:\n",
    "        candidates = [Ngram(context.w_1, context.w_2, next_word.w_1) for next_word in ngram_corpus.unigram_counts.keys()]\n",
    "\n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "    \n",
    "    return next_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3ddd6db53d92002e3f10a5dd731f5980",
     "grade": true,
     "grade_id": "cell-f833625d4520a295",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test_sequence = 'the product'\n",
    "ids = [token_to_id[w] for w in test_sequence.split()]\n",
    "ngram = Ngram(*ids)\n",
    "\n",
    "next_word = predict_next_word(ngram, blt_ngram_corpus)\n",
    "id_to_token = {id_: token for token, id_ in token_to_id.items()}\n",
    "assert id_to_token[next_word] == 'is'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eed3335526e847c86d211248ccf07335",
     "grade": false,
     "grade_id": "cell-c1255f7edc4be0c8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "stop_token_id = token_to_id['</s>']\n",
    "def predict_sentence(start_context: List[int], ngram_corpus: NgramCorpus, limit: int = 5) -> List[int]:\n",
    "    \"\"\"Return a sentence of `len(start_context)`+`limit` words \n",
    "    predicted from `start_context` based on `ngram_corpus`.\"\"\"\n",
    "    if len(start_context) >= 2:\n",
    "        # Create a bigram to start the prediction\n",
    "        ngram_context = Ngram(start_context[-2], start_context[-1])\n",
    "    else:\n",
    "        ngram_context = Ngram(start_context[-1]) # just the last word as a unigram\n",
    "\n",
    "    predicted_sequence = start_context  # Initialize the sequence with the context\n",
    "    \n",
    "    # Hint: use the highest-order ngram you can to predict the next word\n",
    "    # (which would be bigram)\n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "\n",
    "    return predicted_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "79f1cf0f61e9b6452668fcecbe10dea2",
     "grade": false,
     "grade_id": "cell-e09e378a2336939c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test_sequence = 'the product'\n",
    "ids = [token_to_id[w] for w in test_sequence.split()]\n",
    "\n",
    "# Cool!\n",
    "sentence = predict_sentence(ids, blt_ngram_corpus)\n",
    "print(sentence)\n",
    "print(' '.join(id_to_token[id_] for id_ in sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2aa93778f84b6d9ce62a2dcd786ed50f",
     "grade": false,
     "grade_id": "cell-c7c40aefd18264ae",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def perplexity(test_sequence: List[int], ngram_corpus: NgramCorpus) -> float:\n",
    "    \"\"\"Return the perplexity of `test_sequence` given `train_corpus` using trigrams\n",
    "    whenever possible.\"\"\"\n",
    "\n",
    "    product = 1\n",
    "    # First word\n",
    "    product *= 1 / ngram_probability(Ngram(test_sequence[0]), ngram_corpus)\n",
    "    # First two words\n",
    "    product *= 1 / ngram_probability(Ngram(test_sequence[0], test_sequence[1]), ngram_corpus)\n",
    "    # your code here\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bfd40aa753e7679478fd4e3813d676c0",
     "grade": true,
     "grade_id": "cell-4a4e46b4c4c9dcb7",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test_sentence_1 = 'the product is not worth the money.'\n",
    "test_sentence_2 = 'egyd hr fhjfhjtjr rhjtjt tjfhjfwettert'\n",
    "ids1 = [token_to_id[w] for w in test_sequence_1.split()]\n",
    "ids2 = [token_to_id[w] for w in test_sequence_2.split()]\n",
    "\n",
    "assert perplexity(ids1, blt_ngram_corpus) < perplexity(ids2, blt_ngram_corpus)\n",
    "print('Woohoo, it works!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5d1bf3f8f5151c42083c453c3e6bc21b",
     "grade": false,
     "grade_id": "cell-7a8d7411e258baab",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Part 2: Sentiment\n",
    "\n",
    "Let's see if we can use our ngram language modeling techniques to predict attributes of this data. Let's build two different ngram models: one for the positive reviews, and one for the negative reviews. We want to make sure we can evaluate our work, so let's reserve some reviews as a test set first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a6823082346280f4b948363861c855a8",
     "grade": false,
     "grade_id": "cell-636f3dcc34b66edf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# We'll use sklearn to reserve 20% of our data as a test set:\n",
    "train_reviews, test_reviews, train_sentiment, test_sentiment = train_test_split(\n",
    "    tokenized_reviews, \n",
    "    sentiment, \n",
    "    test_size=0.2, \n",
    "    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b48d7c96b514d0222efe0ef3a9220f72",
     "grade": false,
     "grade_id": "cell-e0a059e760495de5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Split apart the positive and negative reviews\n",
    "\n",
    "positive_indices = [i for i in range(len(train_sentiment)) if train_sentiment[i] == 'pos']\n",
    "negative_indices = [i for i in range(len(train_sentiment)) if train_sentiment[i] == 'neg']\n",
    "\n",
    "positive_reviews = [train_reviews[i] for i in positive_indices]\n",
    "negative_reviews = [train_reviews[i] for i in negative_indices]\n",
    "\n",
    "print('Size of training set:', len(train_reviews))\n",
    "print('Size of test set:', len(test_reviews))\n",
    "print('# positive reviews in training set:', len(positive_reviews))\n",
    "print('# negative reviews in training set:', len(negative_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c26e74a811945df714fdafd409c1b3fc",
     "grade": false,
     "grade_id": "cell-ccc6ed2f19b44050",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Now, let's make our two langauge models. One is trained on only\n",
    "# positive reviews, and  one is trained on only negative reviews.\n",
    "positive_ngrams = NgramCorpus(n = 3, tokenized_data = positive_reviews)\n",
    "negative_ngrams = NgramCorpus(n = 3, tokenized_data = negative_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f8d334ec24087bad8766a9f59d4babe6",
     "grade": false,
     "grade_id": "cell-f633835ceb691ee8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's play around with these a bit. Are they doing what we\n",
    "# think they should be doing?\n",
    "test_sequence = 'the'\n",
    "ids = [token_to_id[w] for w in test_sequence.split()]\n",
    "\n",
    "sentence = predict_sentence(ids, positive_ngrams, limit = 6)\n",
    "print('---------------------------------------------')\n",
    "print('A positive-sounding sentence:')\n",
    "print(' '.join(id_to_token[id_] for id_ in sentence))\n",
    "print('---------------------------------------------')\n",
    "\n",
    "test_sequence = 'the'\n",
    "ids = [token_to_id[w] for w in test_sequence.split()]\n",
    "sentence = predict_sentence(ids, negative_ngrams, limit = 6)\n",
    "print('A negative-sounding sentence:')\n",
    "print(' '.join(id_to_token[id_] for id_ in sentence))\n",
    "print('---------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "796bc618239284d335d76af23127c810",
     "grade": false,
     "grade_id": "cell-c3a05d548c3eb9b5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def classify_sentiment(ids: List[int]):\n",
    "    \"\"\"A function that classifies the sentiment of text using our language models.\"\"\"\n",
    "    positive_perplexity = perplexity(ids, positive_ngrams)\n",
    "    negative_perplexity = perplexity(ids, negative_ngrams)\n",
    "    \n",
    "    if positive_perplexity < negative_perplexity:\n",
    "        return 'pos'\n",
    "    else:\n",
    "        return 'neg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classify_sentiment([token_to_id[w] for w in 'The hotel was wonderful.'.split()]))\n",
    "print(classify_sentiment([token_to_id[w] for w in 'That hotel stinks!'.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f2c59ba10e94ab9a5ed78c1db03619c6",
     "grade": false,
     "grade_id": "cell-dec3877362804f0a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Wow, it looks like it is working! Let's do an official test, \n",
    "# using our held-out test set...\n",
    "positive_test_indices = [i for i in range(len(test_sentiment)) if test_sentiment[i] == 'pos']\n",
    "negative_test_indices = [i for i in range(len(test_sentiment)) if test_sentiment[i] == 'neg']\n",
    "\n",
    "positive_test_reviews = [test_reviews[i] for i in positive_test_indices]\n",
    "negative_test_reviews = [test_reviews[i] for i in negative_test_indices]\n",
    "\n",
    "print('# positive reviews in test set:', len(positive_test_reviews))\n",
    "print('# negative reviews in test set:', len(negative_test_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cd78e1f2d4f06f48964e789d49d13745",
     "grade": true,
     "grade_id": "cell-f0cb679a8b041ea1",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "tp = 0\n",
    "fn = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "\n",
    "for pos_rev in positive_test_reviews:\n",
    "    prediction = classify_sentiment(pos_rev)\n",
    "    if prediction == 'pos':\n",
    "        tp += 1\n",
    "    else:\n",
    "        fn += 1\n",
    "        \n",
    "for neg_rev in negative_test_reviews:\n",
    "    prediction = classify_sentiment(neg_rev)\n",
    "    if prediction == 'neg':\n",
    "        tn += 1\n",
    "    else:\n",
    "        fp += 1\n",
    "        \n",
    "positive_acc = tp/(tp + fn)\n",
    "negative_acc = tn/(tn + fp)\n",
    "total_acc = (tp + tn) / (tp + tn + fn + fp)\n",
    "print(f'Accuracy on positive reviews: {round(positive_acc * 100, 2)}% correct')\n",
    "print(f'Accuracy on negative reviews: {round(negative_acc * 100, 2)}% correct')\n",
    "print(f'Total accuracy: {round(total_acc * 100, 2)}% correct')\n",
    "\n",
    "assert total_acc > 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "88cb1f0f55ec98fce1e108008c77f4d5",
     "grade": false,
     "grade_id": "cell-f48121cf933164ad",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "With this technique and this data, you should be able to achieve better than random (50%) accuracy, but maybe not much more than that! In the next homework, we'll explore techniques that will allow us to classify sentiment with greater accuracy. In the meantime, take some time to mull over this assignment. Why do you think this technique is so much more effective on negative reviews than on positive reviews? Do you think there's any hope for this technique working better, and how could we make it work better, if so?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
