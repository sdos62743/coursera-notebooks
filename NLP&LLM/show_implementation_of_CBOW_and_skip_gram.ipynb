{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Implementing the full Skip-gram and CBOW models from scratch in PyTorch involves defining the architecture, the custom loss function (usually **Negative Sampling** for efficiency), and the training loop.\n",
        "\n",
        "Since the basic architecture for both is a very shallow neural network, the implementations are quite similar. We'll use **Negative Sampling** as the loss function, as standard Softmax is computationally prohibitive for large vocabularies.\n",
        "\n",
        "-----\n",
        "\n",
        "## ðŸ› ï¸ 1. Setup and Helper: Negative Sampling Loss\n",
        "\n",
        "Both Skip-gram and CBOW rely on the same fundamental mechanism for efficient training: **Negative Sampling Loss**. This avoids calculating gradients for the entire vocabulary."
      ],
      "metadata": {
        "id": "EB52Qq5u6hkt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class NegativeSamplingLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements the Negative Sampling loss function for Word2Vec.\n",
        "    It maximizes the similarity of the target/context pair (positive sample)\n",
        "    and minimizes the similarity of the target/negative pairs.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, positive_score, negative_score):\n",
        "        # 1. Positive Loss: Maximize the probability of the correct pair (Sigmoid close to 1)\n",
        "        # We maximize log(sigma(x)), which is equivalent to minimizing -log(sigma(x))\n",
        "        positive_loss = F.logsigmoid(positive_score)\n",
        "\n",
        "        # 2. Negative Loss: Minimize the probability of incorrect pairs (Sigmoid close to 0)\n",
        "        # We maximize log(sigma(-x)), which is equivalent to minimizing -log(sigma(-x))\n",
        "        negative_loss = F.logsigmoid(-negative_score).sum(1)\n",
        "\n",
        "        # Total loss is the mean of the combined positive and negative losses\n",
        "        # Note: We take the negative because we minimize the negative log likelihood.\n",
        "        return -(positive_loss + negative_loss).mean()\n",
        "\n",
        "# Loss module initialized globally\n",
        "criterion = NegativeSamplingLoss()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "C-iYQKsF6hkw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "## 2\\. Skip-gram Implementation (Target -\\> Context)\n",
        "\n",
        "The Skip-gram model predicts the context words from a single target word."
      ],
      "metadata": {
        "id": "D6xWLYK06hkx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SkipGramModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "        # Input Embedding Matrix (the word vectors we learn)\n",
        "        self.input_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # Output Embedding Matrix (used to score against context/negative words)\n",
        "        self.output_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # Initialization helps stability\n",
        "        self.input_embeddings.weight.data.uniform_(-1, 1)\n",
        "        self.output_embeddings.weight.data.uniform_(-1, 1)\n",
        "\n",
        "    def forward(self, target_word_indices, context_word_indices, negative_word_indices):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            target_word_indices: Indices of the central words (B,)\n",
        "            context_word_indices: Indices of the positive context words (B,)\n",
        "            negative_word_indices: Indices of the negative samples (B, N)\n",
        "        \"\"\"\n",
        "        # 1. Get Embeddings for Target and Context (Input Embeddings)\n",
        "        target_v = self.input_embeddings(target_word_indices) # (B, E)\n",
        "        context_v = self.output_embeddings(context_word_indices) # (B, E)\n",
        "\n",
        "        # 2. Get Embeddings for Negative Samples (Output Embeddings)\n",
        "        negative_v = self.output_embeddings(negative_word_indices) # (B, N, E)\n",
        "\n",
        "        # 3. Calculate Positive Score (Target . Context)\n",
        "        # (B, E) @ (B, E) -> (B, 1) -> Sigmoid is applied in the loss function\n",
        "        positive_score = torch.sum(target_v * context_v, dim=1)\n",
        "\n",
        "        # 4. Calculate Negative Scores (Target . Negative)\n",
        "        # Need to reshape target_v for broadcasting: (B, 1, E)\n",
        "        target_v = target_v.unsqueeze(1)\n",
        "\n",
        "        # (B, 1, E) @ (B, N, E) -> (B, N)\n",
        "        negative_score = torch.bmm(target_v, negative_v.transpose(1, 2)).squeeze(1)\n",
        "\n",
        "        return positive_score, negative_score"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "7Agg9h9s6hkx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "## 3\\. CBOW Implementation (Context -\\> Target)\n",
        "\n",
        "The CBOW model predicts the target word from an **average** of its context words."
      ],
      "metadata": {
        "id": "a1ewsU_R6hky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CBOWModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "        # Input Embedding Matrix\n",
        "        self.input_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # Output Embedding Matrix\n",
        "        self.output_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        self.input_embeddings.weight.data.uniform_(-1, 1)\n",
        "        self.output_embeddings.weight.data.uniform_(-1, 1)\n",
        "\n",
        "    def forward(self, context_word_indices, target_word_indices, negative_word_indices):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            context_word_indices: Indices of the context words (B, C) where C is context size * 2\n",
        "            target_word_indices: Indices of the target words (B,)\n",
        "            negative_word_indices: Indices of the negative samples (B, N)\n",
        "        \"\"\"\n",
        "        # 1. Get Embeddings for Context Words (Input Embeddings)\n",
        "        context_v = self.input_embeddings(context_word_indices) # (B, C, E)\n",
        "\n",
        "        # 2. Average the Context Embeddings to get the Context Vector (X_c)\n",
        "        # This is the 'Bag-of-Words' step.\n",
        "        context_vector = torch.mean(context_v, dim=1) # (B, E)\n",
        "\n",
        "        # 3. Get Embeddings for Target and Negative Samples (Output Embeddings)\n",
        "        target_v = self.output_embeddings(target_word_indices) # (B, E)\n",
        "        negative_v = self.output_embeddings(negative_word_indices) # (B, N, E)\n",
        "\n",
        "        # 4. Calculate Positive Score (Context Vector . Target)\n",
        "        # (B, E) * (B, E) -> (B, 1)\n",
        "        positive_score = torch.sum(context_vector * target_v, dim=1)\n",
        "\n",
        "        # 5. Calculate Negative Scores (Context Vector . Negative)\n",
        "        context_vector = context_vector.unsqueeze(1) # (B, 1, E)\n",
        "\n",
        "        # (B, 1, E) @ (B, N, E) -> (B, N)\n",
        "        negative_score = torch.bmm(context_vector, negative_v.transpose(1, 2)).squeeze(1)\n",
        "\n",
        "        return positive_score, negative_score"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "nRJfMoZR6hky"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Note on Training Data\n",
        "\n",
        "Both implementations require that your data pipeline generates the following tuples for each batch:\n",
        "\n",
        "  * **Skip-gram:** `(target_word, context_word, [negative_samples])`\n",
        "  * **CBOW:** `([context_words], target_word, [negative_samples])`\n",
        "\n",
        "This data structure is crucial for the models to learn correctly using the negative sampling loss function."
      ],
      "metadata": {
        "id": "3VRp_CIx6hky"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}